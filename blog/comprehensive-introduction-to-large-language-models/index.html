<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>EgorDm's Blog ‚Ä¢ A Comprehensive Introduction to Large Language Models</title><link href=static/img/avatar.jpg rel=icon type=image/png><link title="EgorDm's Blog" href=https://egordmitriev.dev/atom.xml rel=alternate type=application/atom+xml><link href=https://egordmitriev.dev/custom_subset.css rel=stylesheet><link href=https://egordmitriev.dev/main.css media=screen rel=stylesheet><meta name=description><meta content="index, nofollow" name=robots><meta content="EgorDm's Blog" property=og:title><meta content=article property=og:type><meta content=https://egordmitriev.dev/blog/comprehensive-introduction-to-large-language-models/ property=og:url><meta property=og:description><meta content="EgorDm's Blog" property=og:site_name><meta content="default-src 'self';font-src 'self' data: egordm.github.io;img-src 'self' https://* data: egordm.github.io;script-src 'self' egordm.github.io;style-src 'self' egordm.github.io;frame-src player.vimeo.com https://www.youtube-nocookie.com egordm.github.io" http-equiv=Content-Security-Policy><script src=https://egordmitriev.dev/js/initialize_theme_min.js></script><script defer src=https://egordmitriev.dev/js/main_min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://egordmitriev.dev>EgorDm's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://egordmitriev.dev>home</a><li><a class="nav-links no-hover-padding" href=https://egordmitriev.dev/blog>blog</a><li><a class="nav-links no-hover-padding" href=https://egordmitriev.dev/archive>archive</a><li><a class="nav-links no-hover-padding" href=https://egordmitriev.dev/tags>tags</a><li class=theme-switcher-wrapper><div class=theme-switcher></div></ul></div></nav></header><div class=content><main><article><div class=article-title>A Comprehensive Introduction to Large Language Models</div><ul class=meta><li>16th Dec 2023 ‚Ä¢<li title="3871 words">¬†20 min read<li>¬†‚Ä¢¬†Tags:¬†<li><a href=https://egordmitriev.dev/tags/machine-learning/>machine learning</a>,¬†<li><a href=https://egordmitriev.dev/tags/ai/>AI</a>,¬†<li><a href=https://egordmitriev.dev/tags/llm/>LLM</a></ul><section class=body><small> Originaly published as part of <a href=https://www.luminis.eu/nl/blog/llm-series-part-1-a-comprehensive-introduction-to-large-language-models/ target=_blank>Luminis AI Blog</a>. </small><h2 id=introduction><a aria-label="Anchor link for: introduction" class=zola-anchor href=#introduction>üîó</a>Introduction</h2><p>Large language models (LLMs) are all the buzz these days. From big corporations like Microsoft enhancing their office products to Snapchat having an assistant for entertainment, to high schoolers trying to cheat their assignments. Everyone is trying to incorporate LLMs into their products, services, and workflow.<p>With the surge in popularity, there's a flurry of discussions, blogs, and news articles about fine-tuning these models and their myriad applications. Even if you've dipped your toes into the LLM pool, you might find yourself stumbling upon unfamiliar terms and concepts.<p>In this three-part blog series, we'll map out all the key concepts related to LLMs, so you can finally understand what your ML enthusiast colleague is talking about, but also potentially incorporate these powerful models into your projects. So, buckle up, and let's dive into the fascinating world of Large Language Models!<h2 id=prior-knowledge><a aria-label="Anchor link for: prior-knowledge" class=zola-anchor href=#prior-knowledge>üîó</a>Prior Knowledge</h2><p>Before we dive deeper into the key LLM concepts, it's helpful to cover some foundational background knowledge. This will ensure we're all on the same page as we explore the intricacies of LLMs.<h3 id=tokens><a aria-label="Anchor link for: tokens" class=zola-anchor href=#tokens>üîó</a>Tokens</h3><p>Let's start with a simple question: how would you split a sentence into words? Seems straightforward, right? But what if the sentence uses contractions like "can't" or "I'm"? And what if we switch to a different language, say, Swedish or Polish? Would you split it the same way?<p>This is where the concept of "tokenization" comes into play. It's all about splitting text into smaller, discrete units (or "<strong>tokens</strong>"), preferably, in a reversible way. This provides a neat, organized way for our models to process the text.<p>One of the key properties of tokens is that they belong to a fixed-size set, aptly named the "<strong>vocabulary</strong>". This makes them much easier to work with mathematically. Each token can be represented by its unique ID in the set or as a one-hot vector.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=248 src=https://egordmitriev.dev/processed_images/Tokenization_and_Embedding.bcffa104e82b5adf.png width=500><figcaption class=image-captions-caption><p>The document is tokenized and one-hot encoded producing a fixed-size matrix of vectors. These vectors are fed through a function that transforms them into embeddings, effectively reducing the dimensionality</p></figcaption></figure><p>In the olden days, tokenizers were quite lossy. It was common to work on stemmed words and only consider a set of the most common words. Modern tokenizers, instead, have evolved to focus on efficiency and losslessness. Instead of encoding whole words, algorithms such as <a href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt" rel=noopener target=_blank>Byte Pair Encoding (BPE)</a> take a compression-like approach by breaking words apart.<p>Vocabulary construction is done in a purely data-driven manner, resulting in token splits that make sense semantically, such as the common verb ending "-ing". Words like "working", "eating", and "learning" all share this ending, thus an efficient encoding is to give "-ing" its own token. Some splits don't make sense, producing semantically dissimilar tokens such as "lab-elling" which requires the model to do more work to infer its true meaning.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=235 src=https://egordmitriev.dev/processed_images/sentence-tokenization.1d941b2ed7779668.png width=600><figcaption class=image-captions-caption><p>OpenAI's <code>cl100k_base</code> tokenizer encoding a sentence. In this case, 100k refers to its vocabulary size.</p></figcaption></figure><p>But what if a word doesn't exist in the vocabulary? Like "w0rk1ng"? In this case, the tokenizer breaks it down into smaller chunks, sometimes even character by character.<p>Now, it's tempting to assume that a token corresponds to a word. And while that's often the case, it's not a hard and fast rule. For simplicity's sake, we'll often use the terms "word" and "token" interchangeably in this series. But remember, in the wild world of tokenization, a token could be anything from a whole word to a single character or a common word part.<p>For ballpark estimates of token count, you can multiply your word count by 1.25 (assuming English text). This gives a reasonable approximation of the number of tokens in a given piece of text. However, if you're looking for a more precise estimate, you can use the <a href=https://platform.openai.com/tokenizer rel=noopener target=_blank>OpenAI tokenizer web tool</a>.<h3 id=token-embeddings><a aria-label="Anchor link for: token-embeddings" class=zola-anchor href=#token-embeddings>üîó</a>Token Embeddings</h3><p>Now that we've got our tokens, we need a way to represent them that captures more than just their identity. We've seen that tokens can be represented as one-hot vectors, which are great for basic math but not so helpful when it comes to comparing different words together. They don't capture the nuances of language, like how "cat" and "dog" are more similar to each other than "cat" and "car".<p>Enter the concept of "<strong>token embeddings</strong>", also known as "word vectors". The seminal paper <a href=https://arxiv.org/abs/1301.3781 rel=noopener target=_blank>Word2Vec</a> was instrumental in bringing this concept to the mainstream. The authors built on two key assumptions from prior work:<ol><li>Similar words occur in the same context (a concept known as <a href=https://en.wikipedia.org/wiki/Distributional_semantics rel=noopener target=_blank>Distributional Semantics</a>).<li>Similar words have similar meanings.</ol><p>It's important to note that these two assumptions are distinct. The first is about the context in which words are used, while the second is about the meanings of the words themselves.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=232 src=https://egordmitriev.dev/processed_images/linear_relationships_in_embeddings.b9a3e9495337dffe.png width=600><figcaption class=image-captions-caption><p>Demonstration of Linear Relationships Between Words Visualized in Two Dimensional Space. Image from <a href=https://cloud.google.com/blog/products/ai-machine-learning/example-based-explanations-to-build-better-aiml-models rel=noopener target=_blank>Google Blog</a></p></figcaption></figure><p>At first glance, "similarity" might seem like a subjective concept. But what if we think of words as high-dimensional vectors? Suddenly, similarity becomes a very concrete concept. It could be the L2 distance between vectors, the cosine similarity, the dot product similarity, and so on.<p>In the Word2Vec paper, the authors used gradient descent techniques to find embeddings for each word. The goal was to ensure that the above assumptions hold true when comparing these embeddings. In other words, they wanted to find a way to represent words as vectors in a high-dimensional space such that similar words (in terms of context and meaning) are close together in that space.<p>This was a game-changer in the field of natural language processing. Suddenly, we had a way to capture the richness and complexity of language in a mathematical form that machines could understand and work with.<p>But the real beauty of these embeddings is that they can capture relationships between words. For example, the vector difference between "king" and "queen" is similar to the difference between "man" and "woman". This suggests that the embeddings have learned something about the concept of gender.<p>However, it's important to remember that these embeddings are not perfect. They are learned from data, and as such, they can reflect and perpetuate the biases present in that data. This is an important consideration when using these embeddings in real-world applications.<h3 id=encoders-and-decoders><a aria-label="Anchor link for: encoders-and-decoders" class=zola-anchor href=#encoders-and-decoders>üîó</a>Encoders and Decoders</h3><p>While methods like Word2Vec are great for creating simple word embeddings, they produce what we call "shallow embeddings". In this context, shallow means that a matrix of weights is trained directly, and thus can be used like a dictionary. As such, the number of possible embeddings is equal to the number of tokens in your vocabulary. This works fine when you're dealing with individual words, but it starts to break down when you're working with sentences or whole documents.<p>Why? Well, if you encode all the tokens in a sentence into embeddings, you lose all the order of the words. And as anyone who's ever played a game of "<a href=https://en.wikipedia.org/wiki/Mad_Libs rel=noopener target=_blank>Mad Libs</a>" knows, word order is crucial when it comes to making sense of a sentence. By losing the order, you also lose the context, which can drastically change the meaning of a word.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=295 src=https://egordmitriev.dev/processed_images/sentence_encoding_diagram.0c0c322ccc1d1e23.png width=500><figcaption class=image-captions-caption><p>The encoder produces a document embedding by combining the individual word embeddings</p></figcaption></figure><p>To overcome this limitation, we need an additional model, the "Encoder", which does some neural net math magic to create an embedding that takes both context and order into account.<p>On the other side of the equation, we have the "Decoder". Its job is to produce a token from the input, which is typically a latent vector.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=173 src=https://egordmitriev.dev/processed_images/sequence_encoding_diagram.e20f468378f368dc.png width=600><figcaption class=image-captions-caption><p>Visualization of a encoding a sequence of tokens embeddings into a single latent representation, and decoding it into a sequence of token probabilities.</p></figcaption></figure><p>The architecture of how Encoders and Decoders work can vary greatly. It can be based on Transformers, LSTMs, or a combination of both. We'll dive deeper into these architectures in a later blog.<p>One interesting thing to keep in mind is that, since encoders and decoders operate in latent space, their input is not limited to text. They can also take embeddings produced from images, audio, and other modalities. This is thanks to innovations like <a href=https://openai.com/research/clip rel=noopener target=_blank>CLIP</a>, which are trained on multimodal tasks by introducing an encoder for each data type.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=247 src=https://egordmitriev.dev/processed_images/next_gpt_multimodal_inference.9c48e442986ba285.png width=600><figcaption class=image-captions-caption><p>Inference process on <a href=https://next-gpt.github.io/ rel=noopener target=_blank>NExT-GPT</a> with text, image, audio, and video modalities.</p></figcaption></figure><h2 id=modeling-methods><a aria-label="Anchor link for: modeling-methods" class=zola-anchor href=#modeling-methods>üîó</a>Modeling Methods</h2><p>The architecture of large language models isn‚Äôt the only factor that gives them an edge. How they model natural language processing tasks also contributes greatly to their performance. Rather than taking a one-size-fits-all approach, large language models specialize in different modeling methods optimized for certain tasks.<h3 id=causal-language-models-clm><a aria-label="Anchor link for: causal-language-models-clm" class=zola-anchor href=#causal-language-models-clm>üîó</a>Causal Language Models (CLM)</h3><p>Causal Language Models (CLMs) are trained with an autoregressive objective, which is a fancy way of saying they're trained to predict the next token in a sequence based solely on the previous tokens.<p>CLMs typically work in an unidirectional manner, meaning the next token depends only on the previous tokens. It's a bit like reading a book ‚Äì you don't know what's coming next until you've read what's come before. Their architecture reflects this, as CLMs are typically decoder-only.<p>Because of their autoregressive nature, CLMs are great for tasks like text (and code) completion, chat, and story writing. Examples of CLMs include <a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf rel=noopener target=_blank>Generalized Pretrained Transformer (GPT)</a>, and its derivatives, such as <a href=https://ai.meta.com/llama/ rel=noopener target=_blank>Meta's Llama</a>.<h3 id=masked-language-models-mlm><a aria-label="Anchor link for: masked-language-models-mlm" class=zola-anchor href=#masked-language-models-mlm>üîó</a>Masked Language Models (MLM)</h3><p>On the other hand, Masked Language Models (MLMs) are trained to predict masked tokens in a given input by randomly masking certain tokens during training. Its objective task is to ‚Äúfill in the blanks‚Äù given a sentence, but complementary tasks are also used, like predicting which token has been replaced.<p>Unlike CLMs, MLMs typically use a bidirectional architecture, meaning they use the context on both sides of a word. This gives them a broader perspective and generally leads to a better understanding of the relationships between words.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=233 src=https://egordmitriev.dev/processed_images/llm_attention_comparison.4d64a3775a0abcd6.png width=800><figcaption class=image-captions-caption><p>Differences between attention direction. BERT uses a bi-directional Transformer. OpenAI GPT uses a unidirectional left-to-right Transformer</p></figcaption></figure><p>MLMs are particularly suitable for tasks like text classification, sentiment analysis, and text tagging. <a href=https://www.luminis.eu/blog/search-en/decoding-similarity-search-with-faiss-a-practical-approach/ rel=noopener target=_blank>Semantic search</a> is driven by LLMs, where the mathematical distance between document embeddings us used as distance. However, they don't add much value for incremental token prediction tasks because of their bidirectional nature. Nor can they fill in an arbitrary amount of words.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=566 src=https://egordmitriev.dev/processed_images/llm_architecture_comparison.5924cb7384ec2361.png width=800><figcaption class=image-captions-caption><p>Comparison between architectures of influential models from different modelling methods. (from left to right) BERT is an MLM, Original Transformer is a Seq2Seq model, and LLaMA is a CLM.</p></figcaption></figure><p><a href=https://arxiv.org/abs/1810.04805 rel=noopener target=_blank>BERT (Bidirectional Encoder Representations from Transformers)</a> model is highly effective in document embedding. Both BERT and <a href=https://arxiv.org/abs/1802.05365 rel=noopener target=_blank>ELMo (Embeddings from Language Models)</a> have been instrumental in advancing the field of natural language processing and continue to be widely used in a variety of applications.<h3 id=sequence-to-sequence-models-seq2seq><a aria-label="Anchor link for: sequence-to-sequence-models-seq2seq" class=zola-anchor href=#sequence-to-sequence-models-seq2seq>üîó</a>Sequence-to-Sequence Models (Seq2Seq)</h3><p>The Sequence-to-Sequence models (Seq2Seq) aim to transform an input sequence (source) into a new one (target), and both sequences can be of arbitrary lengths. Intuitively, it works like translating a sentence from one language to another ‚Äì the input and output sentences don't have to be the same length, but they do relate to one another.<p>Seq2Seq models are typically composed of an encoder-decoder architecture, which can be based on Transformers or Recursive Neural Networks (RNNs). The encoder processes the input sequence and compresses it into a latent representation, and the decoder then generates the output sequence from this representation. It is a common sentiment that RNN-based models, while being more expensive (and poorly parallelizable)32, are better than transformer-only models. Thus, various works such as <a href=https://github.com/BlinkDL/RWKV-LM rel=noopener target=_blank>RWKV</a> try to combine the best of both worlds to create hybrid models.<p>These models can generally generate coherent, much larger output based on input, making them suitable for tasks like summarization, translation, and question answering.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=159 src=https://egordmitriev.dev/processed_images/encoder_decoder_diagram.84370ad922f8e946.png width=600><figcaption class=image-captions-caption><p>Visualization of encoding and decoding flow of an Seq2Seq model.</p></figcaption></figure><p>A popular example of a Seq2Seq model is <a href=https://arxiv.org/abs/1910.10683 rel=noopener target=_blank>T5 (Text-to-Text Transfer Transformer)</a> which during training frames all NLP tasks (such as translation, classification, summarization, and more) into text-to-text problems. Doing so, allows it to learn patterns useful for a variety of tasks. Another popular example is <a href=https://arxiv.org/abs/1910.13461 rel=noopener target=_blank>BART (Bidirectional and Auto-Regressive Transformers)</a> which is pre-trained by corrupting text and forcing it to reconstruct the original, which improves its text comprehension. These models have shown impressive results on a wide range of tasks, with only a fraction of parameters <a href=https://declare-lab.net/instruct-eval/ rel=noopener target=_blank>they can outperform CLMs on various tasks</a>.<h2 id=the-current-state-of-the-art><a aria-label="Anchor link for: the-current-state-of-the-art" class=zola-anchor href=#the-current-state-of-the-art>üîó</a>The Current State-of-the-Art</h2><p>In the current landscape of large language models (LLM), <a href=https://arxiv.org/abs/1706.03762 rel=noopener target=_blank>transformer-based architectures</a> largely steal the limelight. If you are already wondering what‚Äôs working under the hood, we're planning on taking a deeper dive into their components in the next blog.<p>The ever-growing families of models and variants for architectures like <a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf rel=noopener target=_blank>GPT</a> or <a href=https://arxiv.org/abs/1810.04805 rel=noopener target=_blank>BERT</a> would cause anyone a headache to keep up. Besides the model architecture and [[#Tasks|modeling methods]] we have the label of foundational models to help us further organize our taxonomy. The <strong>foundational models</strong> are the Swiss army knives of AI models. Unlike conventional AI systems, they are trained broadly to be adapted to a variety of tasks with minimal labeled data. The core idea is that if you need more performance at a specialized task, you can start fine-tuning from a solid basis and not from scratch.<p>There are now many commercial and open-source options available for Causal Language Models (CLMs). Notable commercial CLMs include <a href=https://platform.openai.com/docs/guides/gpt rel=noopener target=_blank>OpenAI's GPT</a>, <a href=https://ai.google/discover/palm2/ rel=noopener target=_blank> Google's PaLM</a> and <a href=https://www.anthropic.com/index/claude-2 rel=noopener target=_blank>Anthropic's Claude</a>. GPT-4 is a particularly impressive model, with an ensemble of 8 models, each with 220 billion weights. It amounts to an effective size of 1.7 trillion parameters while providing reasonable latency.<p>On the open-source side, Meta's <a href=https://ai.meta.com/llama/ rel=noopener target=_blank>Meta's LLaMA</a> and <a href=https://mistral.ai/ rel=noopener target=_blank>Mistral</a> have gained significant popularity. LLaMA models are available in a range of sizes, from 7 to 70 billion weights. This gives companies the flexibility to choose the model that best fits their needs or to fine-tune it themselves. The community has also developed many tools and optimizations to facilitate running LLaMA.<p>When picking your model, one should always consider the use case and the amount of effort you are willing to spend on it. There are various benchmarks such as <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard rel=noopener target=_blank>Huggingface's Open LLM Leaderboard</a> and <a href=https://huggingface.co/spaces/mteb/leaderboard rel=noopener target=_blank>Massive Text Embedding Benchmark (MTEB) Leaderboard</a> that evaluate both open source and commercial models performance an various tasks.<p>For open source models, however, it's worth noting that both LLaMA and Mistral models are trained on English text corpus, potentially impacting their performance on tasks in languages other than English.<h2 id=addressing-limitations-of-llms><a aria-label="Anchor link for: addressing-limitations-of-llms" class=zola-anchor href=#addressing-limitations-of-llms>üîó</a>Addressing Limitations of LLMs</h2><p>As exciting as Large Language Models (LLMs) may be, they're not a one-size-fits-all solution. Just like you wouldn't use a hammer to drive a screw, there are many tasks that LLMs are well-suited for, and equally as many that they aren't. Let's take a look at some limitations posed with LLMs and what techniques exist to work around.<h3 id=retrieval-augmented-generation><a aria-label="Anchor link for: retrieval-augmented-generation" class=zola-anchor href=#retrieval-augmented-generation>üîó</a>Retrieval Augmented Generation</h3><p>A common challenge with LLMs is their ability ‚Äì or rather, inability ‚Äì to accurately recall things from memory. Despite their impressive capacity, these models don't actually "know" anything. They generate text based on patterns they've learned during training, which can sometimes lead to them making stuff up (a phenomenon referred to as <em>"hallucination"</em>).<p>To counteract this, we can use techniques like <strong><a href=https://arxiv.org/abs/2005.11401 rel=noopener target=_blank>Retrieval Augmented Generation (RAG)</a></strong>. This approach involves retrieving documents related to a given prompt and feeding them into the LLM to provide the correct context for answering the question.<p>This retrieval process can be done through semantic or vector searches, and the exciting part is, that it can be applied to your custom data as well as external systems like Google Search, essentially giving the LLM a searchable "knowledge base" to draw from.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=355 src=https://egordmitriev.dev/processed_images/RAG_workflow.3b0c36f76c495667.png width=600><figcaption class=image-captions-caption><p>Retrieval Augmented Generation workflow. Image from <a href=https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html rel=noopener target=_blank>AWS Sagemaker Docs</a></p></figcaption></figure><p>LLMs, despite their sophistication, still fall short when it comes to tasks like performing math calculations or executing code. A model won't be able to solve complex mathematical equations or compile and run a piece of Python code without some external help.<p>This is where "Tools" come to the rescue, it is another key concept related to LLMs. This involves connecting the LLM to external programs by exposing their API interface within the input context. LLM can call these tools to perform the specialized tags by writing API calls which are executed as part of the generation process.<p>A prime example of this concept in action is <a href=https://openai.com/blog/chatgpt-plugins rel=noopener target=_blank>ChatGPT plugins</a>, which enhances the capabilities of ChatGPT by allowing it to reach out to a suite of community-made plugins. Similarly, <a href=https://python.langchain.com rel=noopener target=_blank>Langchain</a> is a more developer-focused platform, that creates API abstractions and pre-built blocks to incorporate this functionality into your application.<p>Extending the reach of LLMs even further is the integration of <strong>multiple modalities</strong>, such as vision and audio. These components convert inputs like images or sound into latent representations, a universal language that our LLM understands.<p><a href=https://openai.com/research/clip rel=noopener target=_blank>CLIP</a>, a breakthrough technology from OpenAI, revolutionized the way we bridge the gap between text and images. Similarly, <a href=https://openai.com/research/gpt-4v-system-card rel=noopener target=_blank>GPT-4V(ision)</a> and <a href=https://llava-vl.github.io/ rel=noopener target=_blank>Large Language and Vision Assistant (LLaVA)</a> expand the capabilities of LLMs to comprehend and reason over images.<h3 id=chat-and-agents><a aria-label="Anchor link for: chat-and-agents" class=zola-anchor href=#chat-and-agents>üîó</a>Chat and Agents</h3><p>We have all become familiar with LLMs through user-friendly interfaces like ChatGPT. Traditionally, LLMs provide a single answer as a completion to the input provided. However, various variants are fine-tuned or use prompt engineering to respond in a chat format, allowing for these interactive conversations.<p>To address the limitation of LLMs being limited to single-turn conversations, <strong>AI agents</strong> are designed as systems consisting of multiple LLM agents, each instructed with their specific task. These agents communicate with each other over a chat interface, moderated by AI. By working together, these LLM agents form a collaborative machine that can work towards completing a certain task.<figure class=image-captions-figure><img alt="Anomaly Detection on Row Count quality metric in Soda Cloud" height=329 src=https://egordmitriev.dev/processed_images/llm_agent_chat_workflow.b01d308f4c6666d1.png width=600><figcaption class=image-captions-caption><p>An example of a conversation flow between a python code execution agent, progamming agent and the user. (From <a href=https://github.com/microsoft/autogen rel=noopener target=_blank>AutoGen</a>)</p></figcaption></figure><p>This concept is explored in-depth in the article <a href=https://www.luminis.eu/blog/machine-learning-ai-en/introduction-to-autonomous-agents-in-ai/ rel=noopener target=_blank>Introduction to Autonomous Agents in AI</a>. This collaborative approach has been implemented in projects like <a href=https://github.com/OpenBMB/ChatDev rel=noopener target=_blank>ChatDev</a>, which in true spirit of <a href=https://en.wikipedia.org/wiki/Conway%27s_law rel=noopener target=_blank>Conway's law</a> models agents as a company designed to tackle specific tasks, and <a href=https://github.com/microsoft/autogen rel=noopener target=_blank>Autogen</a> by Microsoft, which provides developer tools to create your agent-based applications.<h3 id=your-own-tasks><a aria-label="Anchor link for: your-own-tasks" class=zola-anchor href=#your-own-tasks>üîó</a>Your Own Tasks</h3><p>There may be instances where you find that LLMs are not producing satisfactory results for your specific task. However, there are several strategies you can employ to address this.<p>One simple trick can be to rephrase your task. The choice of phrasing has a significant influence on how the model responds. Similarly, applying <a href=https://www.promptingguide.ai/ rel=noopener target=_blank><strong>prompt engineering</strong> techniques</a>, like few-shot prompting by providing some examples, can prove useful, giving the model hints about what kind of output you're hoping for.<p>You can also experiment with different completion regimens such as introducing human-in-the-loop agents. This approach mixes AI-generated completions with human guidance to ensure the outputs align with your expectations.<h2 id=conclusion><a aria-label="Anchor link for: conclusion" class=zola-anchor href=#conclusion>üîó</a>Conclusion</h2><p>We've covered a lot of ground in this blog series on large language models. By now, you should have a solid grasp of the key concepts underlying LLMs - their inputs, how to apply them for different tasks, and their capabilities.<p>I hope you've found this exploration illuminating. If you're eager to go deeper into any of the concepts we've discussed, I've included some additional resources below. Feel free to check those out while I work on the next installments.<p>If you have any other questions as you continue your LLM journey, don't hesitate to reach out. I'm always happy to help explain concepts or provide guidance on applying LLMs in business contexts. Whether you need help building an LLM pipeline from scratch, measuring impact and ROI, scaling them up for production, or determining the best use cases for your needs, I'm here. LLMs are powerful tools, but it takes thoughtful implementation to unlock their full potential.<h2 id=resources><a aria-label="Anchor link for: resources" class=zola-anchor href=#resources>üîó</a>Resources</h2><ul><li>Start building RAG systems on AWS: <ul><li><a href=https://www.luminis.eu/nl/blog/search/question-answering-with-your-own-data-llms-and-java-meet-langchain4j/ rel=noopener target=_blank>Question Answering with your own data, LLMs and Java: Meet Langchain4j - Luminis</a><li><a href=https://aws.amazon.com/blogs/machine-learning/improve-llm-responses-in-rag-use-cases-by-interacting-with-the-user/ rel=noopener target=_blank>Improve LLM responses in RAG use cases by interacting with the user | AWS Machine Learning Blog</a></ul><li>Start building multimodal applications: <ul><li><a href=https://www.luminis.eu/nl/blog/search/searching-through-images-using-the-clip-model/ rel=noopener target=_blank>Searching through images using the CLIP model - Luminis</a></ul><li>Awesome LLM Tools: <ul><li><a href=https://www.langchain.com/ rel=noopener target=_blank>LangChain</a><li><a href=https://github.com/microsoft/semantic-kernel rel=noopener target=_blank>GitHub - microsoft/semantic-kernel: Integrate cutting-edge LLM technology quickly and easily into your apps</a></ul><li>Start Building Autonomous LLM Applications: <ul><li>[GitHub - OpenBMB/ChatDev: Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration)](https://github.com/OpenBMB/ChatDev<li><a href=https://github.com/microsoft/autogen rel=noopener target=_blank>GitHub - microsoft/autogen: Building LLM Agent Applications</a></ul><li>Work on your prompts: <a href=https://www.promptingguide.ai/ rel=noopener target=_blank>Prompt Engineering Guide</a></ul></section></article></main></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" rel="noopener noreferrer" href=https://egordmitriev.dev/atom.xml target=_blank> <img alt=atom feed src=https://egordmitriev.dev/social_icons/rss.svg title=atom> </a><li><a class="nav-links no-hover-padding social" rel="noopener noreferrer" href=https://github.com/egordm/ target=_blank> <img alt=github src=https://egordmitriev.dev/social_icons/github.svg title=github> </a><li><a class="nav-links no-hover-padding social" rel="noopener noreferrer" href=https://www.linkedin.com/in/egor-dmitriev/ target=_blank> <img alt=linkedin src=https://egordmitriev.dev/social_icons/linkedin.svg title=linkedin> </a></ul></nav><div class=credits><small>Powered by <a href=https://www.getzola.org target=_blank>Zola</a> & <a href=https://github.com/welpo/tabi target=_blank>tabi</a></small></div></section></footer>